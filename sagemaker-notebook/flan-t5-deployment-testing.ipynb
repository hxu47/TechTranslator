{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30ab5a88-8504-4bea-98cc-7f3c28dd29e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/conda/lib/python3.12/site-packages (4.1.0)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.12/site-packages (1.37.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.37.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.12/site-packages (from boto3) (0.11.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3) (2.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.1.0)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.1->boto3) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy!=1.13.2,>=1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers boto3 numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2de4aeb-07ab-4228-859f-2ee87284ab06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading dependencies and setting up clients...\n",
      "üìù Configuration:\n",
      "  S3 Bucket: tech-translator-s3-knowledge-base\n",
      "  DynamoDB Table: tech-translator-dynamodb-vector-storage\n",
      "ü§ñ Loading sentence transformer model...\n",
      "‚úÖ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Complete Testing Notebook - All Required Functions\n",
    "# Run this cell first to set up everything needed for testing\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import logging\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"üì¶ Loading dependencies and setting up clients...\")\n",
    "\n",
    "# Initialize AWS clients\n",
    "s3 = boto3.client('s3')\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = \"arn:aws:iam::467383999568:role/LabRole\"  #get_execution_role()\n",
    "\n",
    "# Configuration - UPDATE THESE WITH YOUR VALUES\n",
    "BUCKET_NAME = \"tech-translator-s3-knowledge-base\"  # Your S3 bucket\n",
    "TABLE_NAME = \"tech-translator-dynamodb-vector-storage\"  # Your DynamoDB table\n",
    "\n",
    "print(f\"üìù Configuration:\")\n",
    "print(f\"  S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"  DynamoDB Table: {TABLE_NAME}\")\n",
    "\n",
    "# Initialize sentence transformer model\n",
    "print(\"ü§ñ Loading sentence transformer model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# ===== RAG FUNCTIONS (copied from your previous notebook) =====\n",
    "\n",
    "def extract_concept_and_audience(query):\n",
    "    \"\"\"Extract concept and audience from user query\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # Concept mapping\n",
    "    concept_keywords = {\n",
    "        'r-squared': ['r squared', 'r-squared', 'r2', 'coefficient of determination'],\n",
    "        'loss-ratio': ['loss ratio', 'claims ratio', 'incurred losses'],\n",
    "        'predictive-model': ['predictive model', 'prediction model', 'machine learning', 'ml model']\n",
    "    }\n",
    "    \n",
    "    detected_concept = None\n",
    "    for concept_id, keywords in concept_keywords.items():\n",
    "        if any(keyword in query_lower for keyword in keywords):\n",
    "            detected_concept = concept_id\n",
    "            break\n",
    "    \n",
    "    if not detected_concept:\n",
    "        detected_concept = 'predictive-model'  # Default\n",
    "    \n",
    "    # Audience mapping\n",
    "    audience_keywords = {\n",
    "        'underwriter': ['underwriter', 'underwriting'],\n",
    "        'actuary': ['actuary', 'actuarial', 'actuaries'],\n",
    "        'executive': ['executive', 'ceo', 'manager', 'leadership']\n",
    "    }\n",
    "    \n",
    "    detected_audience = None\n",
    "    for audience_id, keywords in audience_keywords.items():\n",
    "        if any(keyword in query_lower for keyword in keywords):\n",
    "            detected_audience = audience_id\n",
    "            break\n",
    "    \n",
    "    if not detected_audience:\n",
    "        detected_audience = 'general'\n",
    "    \n",
    "    return {'concept': detected_concept, 'audience': detected_audience}\n",
    "\n",
    "def vector_search(query, concept_id=None, top_k=5):\n",
    "    \"\"\"Perform vector search on stored embeddings\"\"\"\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = model.encode(query)\n",
    "    \n",
    "    # Query DynamoDB\n",
    "    table = dynamodb.Table(TABLE_NAME)\n",
    "    \n",
    "    try:\n",
    "        if concept_id:\n",
    "            print(f\"üîç Searching in concept: {concept_id}\")\n",
    "            response = table.query(\n",
    "                KeyConditionExpression=\"concept_id = :concept_id\",\n",
    "                ExpressionAttributeValues={\":concept_id\": concept_id}\n",
    "            )\n",
    "        else:\n",
    "            print(\"üîç Searching across all concepts\")\n",
    "            response = table.scan()\n",
    "        \n",
    "        items = response.get('Items', [])\n",
    "        print(f\"Found {len(items)} items to search\")\n",
    "        \n",
    "        if not items:\n",
    "            return []\n",
    "        \n",
    "        # Calculate similarities\n",
    "        results = []\n",
    "        for item in items:\n",
    "            # Parse stored embedding\n",
    "            stored_embedding = json.loads(item['embedding'])\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            similarity = 1 - cosine(query_embedding, stored_embedding)\n",
    "            \n",
    "            results.append({\n",
    "                'item': item,\n",
    "                'similarity': similarity\n",
    "            })\n",
    "        \n",
    "        # Sort by similarity (highest first)\n",
    "        results.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "        \n",
    "        return results[:top_k]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Vector search error: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# ===== DEPLOYMENT FUNCTIONS =====\n",
    "def deploy_model():\n",
    "    \"\"\"Deploy model to SageMaker endpoint \"\"\"\n",
    "    try:\n",
    "        print(\"üì¶ Creating HuggingFace model configuration...\")\n",
    "        \n",
    "        # Model configuration using env parameter\n",
    "        hub = {\n",
    "            'HF_MODEL_ID': 'google/flan-t5-large', #'google/flan-t5-base', #'google/flan-t5-small', \n",
    "            'HF_TASK': 'text2text-generation'\n",
    "        }\n",
    "        \n",
    "        # Create model with supported versions\n",
    "        huggingface_model = HuggingFaceModel(\n",
    "            transformers_version=\"4.37.0\",\n",
    "            pytorch_version=\"2.1.0\",\n",
    "            py_version=\"py310\",\n",
    "            env=hub,\n",
    "            role=role,\n",
    "        )\n",
    "        \n",
    "        print(\"üöÄ Deploying model...\")\n",
    "        print(\"‚è±Ô∏è  This may take 5-10 minutes...\")\n",
    "        \n",
    "        # List of instance types to try\n",
    "        instance_types = [\n",
    "            #\"ml.m5.large\",\n",
    "            #\"ml.c5.large\", \n",
    "            \"ml.m5.xlarge\",\n",
    "            \"ml.c5.xlarge\",\n",
    "        ]\n",
    "        \n",
    "        predictor = None\n",
    "        deployment_successful = False\n",
    "        \n",
    "        for instance_type in instance_types:\n",
    "            try:\n",
    "                print(f\"\\nüîÑ Trying deployment on {instance_type}...\")\n",
    "                \n",
    "                # FIXED: Proper endpoint name parameter\n",
    "                endpoint_name = f\"tech-translator-model-{int(time.time())}\"\n",
    "                \n",
    "                predictor = huggingface_model.deploy(\n",
    "                    initial_instance_count=1,\n",
    "                    instance_type=instance_type,\n",
    "                    endpoint_name=endpoint_name,  \n",
    "                    container_startup_health_check_timeout=600,\n",
    "                    model_data_download_timeout=600,\n",
    "                    wait=True\n",
    "                )\n",
    "                \n",
    "                print(f\"‚úÖ Successfully deployed on {instance_type}!\")\n",
    "                deployment_successful = True\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to deploy on {instance_type}: {str(e)}\")\n",
    "                if any(err in str(e) for err in [\"ResourceLimitExceeded\", \"InsufficientCapacity\", \"ValidationException\"]):\n",
    "                    print(\"   Trying next instance type...\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"   Unexpected error, continuing...\")\n",
    "                    continue\n",
    "        \n",
    "        if deployment_successful:\n",
    "            print(f\"‚úÖ Model deployed successfully!\")\n",
    "            print(f\"üìç Endpoint name: {endpoint_name}\")\n",
    "            return predictor, endpoint_name\n",
    "        else:\n",
    "            print(\"‚ùå All deployment attempts failed!\")\n",
    "            return None, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model creation failed: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cleanup_endpoint(endpoint_name):\n",
    "    \"\"\"Delete the endpoint to avoid charges\"\"\"\n",
    "    print(f\"\\nüßπ Cleaning up endpoint: {endpoint_name}\")\n",
    "    \n",
    "    try:\n",
    "        sagemaker_client = boto3.client('sagemaker')\n",
    "        sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "        print(\"‚úÖ Endpoint deletion initiated!\")\n",
    "        print(\"üí∞ This will stop incurring charges.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error deleting endpoint: {str(e)}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec5a0794-a8a2-457b-937f-7141df8cf953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TESTING FUNCTIONS =====\n",
    "def call_endpoint(endpoint_name, prompt, max_new_tokens=100):\n",
    "    \"\"\"Helper function to call the deployed FLAN-T5 endpoint with proper format\"\"\"\n",
    "    \n",
    "    # FLAN-T5 is a text-to-text model, so we need to format the prompt properly\n",
    "    # It works best with instruction-style prompts\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": max_new_tokens,  # Use max_new_tokens instead of max_length\n",
    "            \"temperature\": 0.7,\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\": 0.9,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            # Remove return_full_text - not supported by FLAN-T5\n",
    "            # Remove pad_token_id - FLAN-T5 handles this automatically\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = sagemaker_runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response['Body'].read().decode())\n",
    "        \n",
    "        # Handle FLAN-T5 response format\n",
    "        if isinstance(result, list) and len(result) > 0:\n",
    "            if isinstance(result[0], dict):\n",
    "                generated_text = result[0].get('generated_text', '')\n",
    "            else:\n",
    "                generated_text = str(result[0])\n",
    "        elif isinstance(result, dict):\n",
    "            generated_text = result.get('generated_text', '')\n",
    "        else:\n",
    "            generated_text = str(result)\n",
    "        \n",
    "        # Clean up the response\n",
    "        generated_text = generated_text.strip()\n",
    "        \n",
    "        return generated_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Endpoint call failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def quick_test(endpoint_name, test_query=\"What is R-squared?\"):\n",
    "    \"\"\"Quick test of the FLAN-T5 endpoint with proper instruction format\"\"\"\n",
    "    print(f\"‚ö° Quick Test: '{test_query}'\")\n",
    "    \n",
    "    # Format the query as an instruction for FLAN-T5\n",
    "    instruction_prompt = f\"Explain the following concept: {test_query}\"\n",
    "    \n",
    "    response = call_endpoint(endpoint_name, instruction_prompt, max_new_tokens=50)\n",
    "    \n",
    "    if response:\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"‚úÖ Endpoint is working!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå Test failed\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_simple_prompts(endpoint_name):\n",
    "    \"\"\"Test with instruction-style prompts optimized for FLAN-T5\"\"\"\n",
    "    print(\"üß™ Testing Simple Prompts for FLAN-T5\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    simple_tests = [\n",
    "        {\n",
    "            \"prompt\": \"Define R-squared in statistics.\",\n",
    "            \"description\": \"R-squared definition\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Explain what loss ratio means in insurance.\",\n",
    "            \"description\": \"Loss ratio explanation\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"Describe how predictive models help insurance companies.\",\n",
    "            \"description\": \"Predictive models in insurance\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, test in enumerate(simple_tests, 1):\n",
    "        print(f\"\\n{i}. {test['description']}\")\n",
    "        print(f\"Prompt: '{test['prompt']}'\")\n",
    "        \n",
    "        response = call_endpoint(endpoint_name, test['prompt'], max_new_tokens=60)\n",
    "        \n",
    "        if response:\n",
    "            print(f\"Response: {response}\")\n",
    "            \n",
    "            # Check if response is relevant\n",
    "            insurance_terms = ['insurance', 'claim', 'premium', 'risk', 'policy', 'coverage']\n",
    "            stats_terms = ['measure', 'ratio', 'model', 'predict', 'data', 'statistical']\n",
    "            \n",
    "            relevant_count = sum(1 for term in insurance_terms + stats_terms if term in response.lower())\n",
    "            \n",
    "            if relevant_count >= 2:\n",
    "                print(\"‚úÖ Good response - contains relevant terms\")\n",
    "            elif relevant_count == 1:\n",
    "                print(\"‚ö†Ô∏è  Okay response - some relevant content\")\n",
    "            else:\n",
    "                print(\"‚ùå Poor response - lacks relevant content\")\n",
    "        else:\n",
    "            print(\"‚ùå No response generated\")\n",
    "        \n",
    "        print(\"-\" * 25)\n",
    "\n",
    "\n",
    "def test_model_basic(predictor):\n",
    "    \"\"\"Basic test of the deployed FLAN-T5 model\"\"\"\n",
    "    print(\"\\nüß™ Running Basic Model Tests for FLAN-T5...\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Test cases optimized for FLAN-T5 instruction format\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"name\": \"Simple Definition\",\n",
    "            \"prompt\": \"Define R-squared in statistics.\",\n",
    "            \"max_new_tokens\": 50\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Insurance Context\",\n",
    "            \"prompt\": \"Explain what loss ratio means in insurance.\",\n",
    "            \"max_new_tokens\": 60\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Concept Explanation\", \n",
    "            \"prompt\": \"What is a predictive model and how is it used?\",\n",
    "            \"max_new_tokens\": 70\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Professional Context\",\n",
    "            \"prompt\": \"Explain how R-squared helps underwriters assess risk models.\",\n",
    "            \"max_new_tokens\": 80\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\nTest {i}: {test_case['name']}\")\n",
    "        print(f\"Input: '{test_case['prompt']}'\")\n",
    "        \n",
    "        try:\n",
    "            # Prepare payload for FLAN-T5 model\n",
    "            payload = {\n",
    "                \"inputs\": test_case[\"prompt\"],\n",
    "                \"parameters\": {\n",
    "                    \"max_new_tokens\": test_case[\"max_new_tokens\"],\n",
    "                    \"temperature\": 0.7,\n",
    "                    \"do_sample\": True,\n",
    "                    \"top_p\": 0.9,\n",
    "                    \"repetition_penalty\": 1.1,\n",
    "                    # No return_full_text or pad_token_id for FLAN-T5\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Call the model\n",
    "            result = predictor.predict(payload)\n",
    "            \n",
    "            # Process result - FLAN-T5 format\n",
    "            if isinstance(result, list) and len(result) > 0:\n",
    "                if isinstance(result[0], dict):\n",
    "                    generated_text = result[0].get('generated_text', 'No text generated')\n",
    "                else:\n",
    "                    generated_text = str(result[0])\n",
    "            elif isinstance(result, dict):\n",
    "                generated_text = result.get('generated_text', 'No text generated')\n",
    "            else:\n",
    "                generated_text = str(result)\n",
    "            \n",
    "            # Clean up the output\n",
    "            generated_text = generated_text.strip()\n",
    "            \n",
    "            print(f\"Output: '{generated_text}'\")\n",
    "            \n",
    "            # Check if output looks reasonable for FLAN-T5\n",
    "            if len(generated_text) > 5 and not generated_text.startswith(\"Error\"):\n",
    "                # Check for relevant terms\n",
    "                relevant_terms = ['ratio', 'model', 'measure', 'data', 'insurance', 'risk', 'statistical']\n",
    "                has_relevant_content = any(term in generated_text.lower() for term in relevant_terms)\n",
    "                \n",
    "                if has_relevant_content:\n",
    "                    print(\"‚úÖ Test passed - relevant content generated!\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è  Test passed but content may not be domain-specific\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Test passed but output seems short\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Test failed: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def test_rag_integration(predictor, endpoint_name):\n",
    "    \"\"\"Test FLAN-T5 with your RAG system using proper instruction formatting\"\"\"\n",
    "    print(\"\\nüîó Testing RAG Integration with FLAN-T5...\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    test_query = \"What is R-squared for an underwriter?\"\n",
    "    \n",
    "    try:\n",
    "        # Extract concept and audience\n",
    "        concept_and_audience = extract_concept_and_audience(test_query)\n",
    "        print(f\"üìä Extracted - Concept: {concept_and_audience['concept']}, Audience: {concept_and_audience['audience']}\")\n",
    "        \n",
    "        # Perform vector search\n",
    "        search_results = vector_search(test_query, concept_and_audience['concept'], top_k=3)\n",
    "        print(f\"üîç Found {len(search_results)} relevant chunks\")\n",
    "        \n",
    "        if not search_results:\n",
    "            print(\"‚ö†Ô∏è  No search results found - make sure your DynamoDB table has data\")\n",
    "            return False\n",
    "        \n",
    "        # Show top results\n",
    "        for i, result in enumerate(search_results[:2]):\n",
    "            print(f\"  {i+1}. {result['item']['vector_id']} (similarity: {result['similarity']:.3f})\")\n",
    "        \n",
    "        # Create context from search results\n",
    "        context_text = \"\"\n",
    "        for result in search_results[:2]:  # Top 2 results\n",
    "            context_text += f\"- {result['item']['text'][:150]}...\\n\"\n",
    "        \n",
    "        # Create instruction-style prompt for FLAN-T5\n",
    "        audience = concept_and_audience['audience']\n",
    "        \n",
    "        # FLAN-T5 works better with clear, direct instructions\n",
    "        prompt = f\"\"\"Based on the following information, explain R-squared to an {audience} in the insurance industry.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {test_query}\n",
    "\n",
    "Provide a clear, professional explanation:\"\"\"\n",
    "        \n",
    "        print(f\"üìù Created instruction prompt ({len(prompt.split())} words)\")\n",
    "        \n",
    "        # Call FLAN-T5\n",
    "        response = call_endpoint(endpoint_name, prompt, max_new_tokens=120)\n",
    "        \n",
    "        if response:\n",
    "            print(f\"ü§ñ FLAN-T5 Response: {response}\")\n",
    "            print(\"‚úÖ RAG integration test passed!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Failed to get FLAN-T5 response\")\n",
    "            return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå RAG integration test failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78b94114-0295-4b83-a217-46b68bdd3315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ All functions loaded successfully!\n",
      "\n",
      "Next steps:\n",
      "1. Run: predictor, endpoint_name = deploy_model()\n",
      "2. Wait for deployment to complete\n",
      "3. Run: test_model_basic(predictor)\n",
      "4. Run: test_rag_integration(predictor, endpoint_name)\n",
      "5. When done: cleanup_endpoint(endpoint_name)\n"
     ]
    }
   ],
   "source": [
    "print(\"üéâ All functions loaded successfully!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Run: predictor, endpoint_name = deploy_model()\")\n",
    "print(\"2. Wait for deployment to complete\")\n",
    "print(\"3. Run: test_model_basic(predictor)\")\n",
    "print(\"4. Run: test_rag_integration(predictor, endpoint_name)\")\n",
    "print(\"5. When done: cleanup_endpoint(endpoint_name)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ce3370-9805-47b2-9db1-81490787714a",
   "metadata": {},
   "source": [
    "# STEP 1: Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e67e434-f9b0-43d7-99ae-e638a06ffc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STEP 1: Deploying model...\n",
      "==================================================\n",
      "üì¶ Creating HuggingFace model configuration...\n",
      "üöÄ Deploying model...\n",
      "‚è±Ô∏è  This may take 5-10 minutes...\n",
      "\n",
      "üîÑ Trying deployment on ml.m5.xlarge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-inference-2025-05-24-14-57-51-240\n",
      "INFO:sagemaker:Creating endpoint-config with name tech-translator-model-1748098670\n",
      "INFO:sagemaker:Creating endpoint with name tech-translator-model-1748098670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!‚úÖ Successfully deployed on ml.m5.xlarge!\n",
      "‚úÖ Model deployed successfully!\n",
      "üìç Endpoint name: tech-translator-model-1748098670\n",
      "\n",
      "‚úÖ Deployment successful!\n",
      "üìç Endpoint: tech-translator-model-1748098670\n",
      "üìù Stored in variable: DEPLOYED_ENDPOINT_NAME\n",
      "\n",
      "‚è≥ Waiting for endpoint to be fully ready...\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Deploy the model\n",
    "print(\"üöÄ STEP 1: Deploying model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "predictor, endpoint_name = deploy_model()\n",
    "\n",
    "if predictor and endpoint_name:\n",
    "    print(f\"\\n‚úÖ Deployment successful!\")\n",
    "    print(f\"üìç Endpoint: {endpoint_name}\")\n",
    "    \n",
    "    # Store for later use\n",
    "    DEPLOYED_ENDPOINT_NAME = endpoint_name\n",
    "    print(f\"üìù Stored in variable: DEPLOYED_ENDPOINT_NAME\")\n",
    "    \n",
    "    # Wait for endpoint to be ready\n",
    "    print(\"\\n‚è≥ Waiting for endpoint to be fully ready...\")\n",
    "    time.sleep(60)  # Wait 1 minute for endpoint to stabilize\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Deployment failed!\")\n",
    "    print(\"Check error messages above and try troubleshooting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baebf80-03f0-4e10-a623-bb54881c3bd6",
   "metadata": {},
   "source": [
    "# STEP 2: Test the deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "929ea105-e759-4646-992c-1fe3674e8582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ STEP 2: Testing the deployed FLAN-T5 model...\n",
      "==================================================\n",
      "‚ö° Quick verification test...\n",
      "‚ö° Quick Test: 'What is R-squared?'\n",
      "Response: r squar\n",
      "‚úÖ Endpoint is working!\n",
      "\n",
      "‚úÖ Basic functionality confirmed!\n",
      "\n",
      "üîç Running comprehensive tests...\n",
      "\n",
      "üß™ Running Basic Model Tests for FLAN-T5...\n",
      "------------------------------\n",
      "\n",
      "Test 1: Simple Definition\n",
      "Input: 'Define R-squared in statistics.'\n",
      "Output: 'r = r * 2'\n",
      "‚ö†Ô∏è  Test passed but content may not be domain-specific\n",
      "\n",
      "Test 2: Insurance Context\n",
      "Input: 'Explain what loss ratio means in insurance.'\n",
      "Output: 'In an insurance policy, a loss ratio is the percentage of a policyholder's loss attributable to insurance policies.'\n",
      "‚úÖ Test passed - relevant content generated!\n",
      "\n",
      "Test 3: Concept Explanation\n",
      "Input: 'What is a predictive model and how is it used?'\n",
      "Output: 'A predictive model is a method for predicting the behavior of an individual or a group of individuals'\n",
      "‚úÖ Test passed - relevant content generated!\n",
      "\n",
      "Test 4: Professional Context\n",
      "Input: 'Explain how R-squared helps underwriters assess risk models.'\n",
      "Output: 'A risk model is a mathematical formula that gives the probability of a given risk event being a metric.'\n",
      "‚úÖ Test passed - relevant content generated!\n",
      "üß™ Testing Simple Prompts for FLAN-T5\n",
      "==============================\n",
      "\n",
      "1. R-squared definition\n",
      "Prompt: 'Define R-squared in statistics.'\n",
      "Response: r squared\n",
      "‚ùå Poor response - lacks relevant content\n",
      "-------------------------\n",
      "\n",
      "2. Loss ratio explanation\n",
      "Prompt: 'Explain what loss ratio means in insurance.'\n",
      "Response: Insurers use loss ratios to calculate how much money they are losing.\n",
      "‚ö†Ô∏è  Okay response - some relevant content\n",
      "-------------------------\n",
      "\n",
      "3. Predictive models in insurance\n",
      "Prompt: 'Describe how predictive models help insurance companies.'\n",
      "Response: Predictive models help insurance companies better understand their customers ' needs and help them decide what to do with the money they get.\n",
      "‚úÖ Good response - contains relevant terms\n",
      "-------------------------\n",
      "\n",
      "üîó Testing RAG integration...\n",
      "\n",
      "üîó Testing RAG Integration with FLAN-T5...\n",
      "------------------------------\n",
      "üìä Extracted - Concept: r-squared, Audience: underwriter\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d17cceb19ba4749923729c3cd184119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching in concept: r-squared\n",
      "Found 8 items to search\n",
      "üîç Found 3 relevant chunks\n",
      "  1. r-squared-underwriter (similarity: 0.727)\n",
      "  2. r-squared-definition (similarity: 0.609)\n",
      "üìù Created instruction prompt (78 words)\n",
      "ü§ñ FLAN-T5 Response: As an underwriter, you can think of R-squared as a measure of how well your pricing model captures risk factors. If your pricing model has an R-squared, you can think of R-squared as a measure of how well your pricing model captures risk factors.\n",
      "‚úÖ RAG integration test passed!\n",
      "\n",
      "üéâ All tests passed! Your TechTranslator system is working!\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Test the deployed model\n",
    "if 'DEPLOYED_ENDPOINT_NAME' in locals() and DEPLOYED_ENDPOINT_NAME:\n",
    "    print(\"üß™ STEP 2: Testing the deployed FLAN-T5 model...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Quick verification first\n",
    "    print(\"‚ö° Quick verification test...\")\n",
    "    if quick_test(DEPLOYED_ENDPOINT_NAME):\n",
    "        print(\"\\n‚úÖ Basic functionality confirmed!\")\n",
    "        \n",
    "        # Run comprehensive tests\n",
    "        print(\"\\nüîç Running comprehensive tests...\")\n",
    "        test_model_basic(predictor)\n",
    "        test_simple_prompts(DEPLOYED_ENDPOINT_NAME)\n",
    "        \n",
    "        # Test RAG integration\n",
    "        print(\"\\nüîó Testing RAG integration...\")\n",
    "        rag_success = test_rag_integration(predictor, DEPLOYED_ENDPOINT_NAME)\n",
    "        \n",
    "        if rag_success:\n",
    "            print(\"\\nüéâ All tests passed! Your TechTranslator system is working!\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  RAG integration has issues - check your DynamoDB data\")\n",
    "    else:\n",
    "        print(\"‚ùå Basic test failed - check the error messages above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6079d2-8563-4e95-ae96-c48c990ce58e",
   "metadata": {},
   "source": [
    "# STEP 3: Advanced testing and demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52d08041-4422-4315-b7a7-b30536211334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ STEP 3: Advanced Testing & Demonstration (FLAN-T5)\n",
      "==================================================\n",
      "\n",
      "üß™ Scenario 1: R-squared explanation for underwriters\n",
      "----------------------------------------\n",
      "User Query: 'What is R-squared for an underwriter?'\n",
      "üìä Concept: r-squared, Audience: underwriter\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a115902ce940b998e3f5db3bc31ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching in concept: r-squared\n",
      "Found 8 items to search\n",
      "üîç Found 3 relevant chunks\n",
      "   Top match: r-squared-underwriter (similarity: 0.727)\n",
      "ü§ñ TechTranslator (FLAN-T5) Response:\n",
      "R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's e.g. the average of the odds of a driver being killed in an accident.\n",
      "‚ö†Ô∏è  Good response - contains relevant domain terms\n",
      "‚úÖ Scenario completed successfully!\n",
      "----------------------------------------\n",
      "\n",
      "üß™ Scenario 2: Loss ratio for executives\n",
      "----------------------------------------\n",
      "User Query: 'Explain loss ratio to an executive'\n",
      "üìä Concept: loss-ratio, Audience: executive\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692ba694f608433aa34ddc45f46a6639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching in concept: loss-ratio\n",
      "Found 8 items to search\n",
      "üîç Found 3 relevant chunks\n",
      "   Top match: loss-ratio-definition (similarity: 0.626)\n",
      "ü§ñ TechTranslator (FLAN-T5) Response:\n",
      "The basic formula is: Loss Ratio = (Incurred Losses + Loss Adjustment Expenses) / Earned Premiums  100%. A combined rat...\n",
      "‚úÖ High-quality response - contains both insurance and technical terms\n",
      "‚úÖ Scenario completed successfully!\n",
      "----------------------------------------\n",
      "\n",
      "üß™ Scenario 3: Predictive models for actuaries\n",
      "----------------------------------------\n",
      "User Query: 'How do predictive models help actuaries?'\n",
      "üìä Concept: predictive-model, Audience: actuary\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068f0fb7777148b285026d27545d0a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching in concept: predictive-model\n",
      "Found 8 items to search\n",
      "üîç Found 3 relevant chunks\n",
      "   Top match: predictive-model-context (similarity: 0.602)\n",
      "ü§ñ TechTranslator (FLAN-T5) Response:\n",
      "A predictive model is a statistical algorithm that uses historical data to predict future outcomes or classify new data.\n",
      "‚ö†Ô∏è  Good response - contains relevant domain terms\n",
      "‚úÖ Scenario completed successfully!\n",
      "----------------------------------------\n",
      "\n",
      "üéØ FLAN-T5 Specific Instruction Tests\n",
      "----------------------------------------\n",
      "\n",
      "üìã Instruction Test 1: Summarization task\n",
      "Instruction: 'Summarize the key benefits of using R-squared in insurance pricing models.'\n",
      "Response: Use R-squared to calculate model parameters.\n",
      "‚úÖ Response generated successfully\n",
      "-------------------------\n",
      "\n",
      "üìã Instruction Test 2: List generation task\n",
      "Instruction: 'List three ways predictive models improve insurance operations.'\n",
      "Response: predict loss.\n",
      "‚ö†Ô∏è  Okay - content relevant but didn't follow list format\n",
      "-------------------------\n",
      "\n",
      "üìã Instruction Test 3: Comparison task\n",
      "Instruction: 'Compare loss ratio and combined ratio in insurance.'\n",
      "Response: The loss ratio is the ratio of the loss to the total insured amount. The combined ratio is the ratio of the combined amount insured to the total insured amount.\n",
      "‚úÖ Response generated successfully\n",
      "-------------------------\n",
      "\n",
      "üèÅ Advanced FLAN-T5 testing completed!\n",
      "\n",
      "üí∞ COST REMINDER:\n",
      "Your FLAN-T5 endpoint 'tech-translator-model-1748098670' is running and incurring charges.\n",
      "Estimated cost: ~$0.19/hour for ml.m5.large\n",
      "Run cleanup_endpoint('tech-translator-model-1748098670') when done!\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Advanced testing and demonstration - Updated for FLAN-T5\n",
    "\n",
    "if 'DEPLOYED_ENDPOINT_NAME' in locals() and DEPLOYED_ENDPOINT_NAME:\n",
    "    print(\"üéØ STEP 3: Advanced Testing & Demonstration (FLAN-T5)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test different insurance scenarios with instruction-style prompts\n",
    "    test_scenarios = [\n",
    "        {\n",
    "            \"query\": \"What is R-squared for an underwriter?\",\n",
    "            \"description\": \"R-squared explanation for underwriters\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"Explain loss ratio to an executive\", \n",
    "            \"description\": \"Loss ratio for executives\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"How do predictive models help actuaries?\",\n",
    "            \"description\": \"Predictive models for actuaries\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, scenario in enumerate(test_scenarios, 1):\n",
    "        print(f\"\\nüß™ Scenario {i}: {scenario['description']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        query = scenario['query']\n",
    "        print(f\"User Query: '{query}'\")\n",
    "        \n",
    "        # Extract concept and audience\n",
    "        concept_and_audience = extract_concept_and_audience(query)\n",
    "        print(f\"üìä Concept: {concept_and_audience['concept']}, Audience: {concept_and_audience['audience']}\")\n",
    "        \n",
    "        # Vector search\n",
    "        search_results = vector_search(query, concept_and_audience['concept'], top_k=3)\n",
    "        \n",
    "        if search_results:\n",
    "            print(f\"üîç Found {len(search_results)} relevant chunks\")\n",
    "            print(f\"   Top match: {search_results[0]['item']['vector_id']} (similarity: {search_results[0]['similarity']:.3f})\")\n",
    "            \n",
    "            # Create context from top results\n",
    "            context_text = \"\"\n",
    "            for result in search_results[:2]:\n",
    "                context_text += f\"- {result['item']['text'][:120]}...\\n\"\n",
    "            \n",
    "            # Create instruction-style prompt optimized for FLAN-T5\n",
    "            audience = concept_and_audience['audience']\n",
    "            concept = concept_and_audience['concept'].replace('-', ' ').title()\n",
    "            \n",
    "            # FLAN-T5 works best with clear, structured instructions\n",
    "            prompt = f\"\"\"You are explaining {concept} to an insurance {audience}. \n",
    "\n",
    "Context information:\n",
    "{context_text}\n",
    "\n",
    "Task: Based on the context above, explain {concept} in a way that an insurance {audience} would understand. Focus on practical applications in insurance.\n",
    "\n",
    "Explanation:\"\"\"\n",
    "            \n",
    "            # Get FLAN-T5 response with appropriate parameters\n",
    "            response = call_endpoint(DEPLOYED_ENDPOINT_NAME, prompt, max_new_tokens=100)\n",
    "            \n",
    "            if response:\n",
    "                print(f\"ü§ñ TechTranslator (FLAN-T5) Response:\\n{response}\")\n",
    "                \n",
    "                # Evaluate response quality\n",
    "                insurance_terms = ['insurance', 'premium', 'risk', 'claim', 'policy', 'underwriting', 'actuarial']\n",
    "                concept_terms = ['ratio', 'model', 'squared', 'predictive', 'statistical', 'measure']\n",
    "                \n",
    "                insurance_count = sum(1 for term in insurance_terms if term.lower() in response.lower())\n",
    "                concept_count = sum(1 for term in concept_terms if term.lower() in response.lower())\n",
    "                \n",
    "                if insurance_count >= 1 and concept_count >= 1:\n",
    "                    print(\"‚úÖ High-quality response - contains both insurance and technical terms\")\n",
    "                elif insurance_count >= 1 or concept_count >= 1:\n",
    "                    print(\"‚ö†Ô∏è  Good response - contains relevant domain terms\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è  Basic response - may lack domain specificity\")\n",
    "                \n",
    "                print(\"‚úÖ Scenario completed successfully!\")\n",
    "            else:\n",
    "                print(\"‚ùå Failed to generate response\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  No relevant chunks found in vector search\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "        time.sleep(2)  # Brief pause between scenarios\n",
    "    \n",
    "    # Additional FLAN-T5 specific tests\n",
    "    print(f\"\\nüéØ FLAN-T5 Specific Instruction Tests\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    instruction_tests = [\n",
    "        {\n",
    "            \"instruction\": \"Summarize the key benefits of using R-squared in insurance pricing models.\",\n",
    "            \"description\": \"Summarization task\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"List three ways predictive models improve insurance operations.\",\n",
    "            \"description\": \"List generation task\"\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"Compare loss ratio and combined ratio in insurance.\",\n",
    "            \"description\": \"Comparison task\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for j, test in enumerate(instruction_tests, 1):\n",
    "        print(f\"\\nüìã Instruction Test {j}: {test['description']}\")\n",
    "        print(f\"Instruction: '{test['instruction']}'\")\n",
    "        \n",
    "        response = call_endpoint(DEPLOYED_ENDPOINT_NAME, test['instruction'], max_new_tokens=80)\n",
    "        \n",
    "        if response:\n",
    "            print(f\"Response: {response}\")\n",
    "            \n",
    "            # Check if FLAN-T5 followed the instruction format\n",
    "            if test['description'] == \"List generation task\":\n",
    "                has_list_format = any(marker in response for marker in ['1.', '2.', '3.', '-', '‚Ä¢'])\n",
    "                if has_list_format:\n",
    "                    print(\"‚úÖ Good - followed list format instruction\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è  Okay - content relevant but didn't follow list format\")\n",
    "            else:\n",
    "                print(\"‚úÖ Response generated successfully\")\n",
    "        else:\n",
    "            print(\"‚ùå No response generated\")\n",
    "        \n",
    "        print(\"-\" * 25)\n",
    "    \n",
    "    print(f\"\\nüèÅ Advanced FLAN-T5 testing completed!\")\n",
    "    \n",
    "    # Cost reminder\n",
    "    print(f\"\\nüí∞ COST REMINDER:\")\n",
    "    print(f\"Your FLAN-T5 endpoint '{DEPLOYED_ENDPOINT_NAME}' is running and incurring charges.\")\n",
    "    print(f\"Estimated cost: ~$0.19/hour for ml.m5.large\")\n",
    "    print(f\"Run cleanup_endpoint('{DEPLOYED_ENDPOINT_NAME}') when done!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No endpoint available - complete Steps 1 and 2 first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94ca5232-5a0b-4710-8bd9-634ad89eca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Improved FLAN-T5 Instruction Tests\n",
      "----------------------------------------\n",
      "\n",
      "üìã Improved Test 1: Improved summarization task\n",
      "Instruction: 'Based on insurance knowledge, summarize in 2-3 sentences: What are the key benefits of using R-squared in insurance pricing models?'\n",
      "Response: In insurance pricing models, R-squared is a key factor in determining the best pricing model.\n",
      "‚úÖ Good - contains relevant insurance terms\n",
      "-------------------------\n",
      "\n",
      "üìã Improved Test 2: Explicit list format task\n",
      "Instruction: 'List exactly 3 ways predictive models improve insurance operations. Format your answer as:\n",
      "1. [first way]\n",
      "2. [second way]\n",
      "3. [third way]'\n",
      "Response: 3.\n",
      "‚úÖ Excellent - followed numbered list format!\n",
      "-------------------------\n",
      "\n",
      "üìã Improved Test 3: Structured comparison task\n",
      "Instruction: 'In insurance, explain the difference between loss ratio and combined ratio. Loss ratio is... Combined ratio is...'\n",
      "Response: The ratio between the amount of insurance paid and the amount of loss.\n",
      "‚úÖ Good - contains relevant insurance terms\n",
      "-------------------------\n",
      "\n",
      "üéØ Context-Enhanced Instruction Tests\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d56c2900f44d7081d256bf4347293a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching in concept: loss-ratio\n",
      "Found 8 items to search\n",
      "üìù Testing context-enhanced prompt...\n",
      "ü§ñ Context-Enhanced Response: 1. Loss ratio is a key insurance metric that measures the relationship between incurred losses and earned premiums, expressed as a percentage. 2. Loss ratio is used to estimate the risk of loss. 3. Loss ratio is used to measure the risk of loss.\n",
      "‚úÖ Excellent - used context AND followed format!\n"
     ]
    }
   ],
   "source": [
    "# Improved instruction tests with better prompts for FLAN-T5\n",
    "def test_improved_instructions(endpoint_name):\n",
    "    \"\"\"Test FLAN-T5 with more explicit, better-structured instructions\"\"\"\n",
    "    print(f\"\\nüéØ Improved FLAN-T5 Instruction Tests\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    improved_tests = [\n",
    "        {\n",
    "            \"instruction\": \"Based on insurance knowledge, summarize in 2-3 sentences: What are the key benefits of using R-squared in insurance pricing models?\",\n",
    "            \"description\": \"Improved summarization task\",\n",
    "            \"max_tokens\": 60\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"List exactly 3 ways predictive models improve insurance operations. Format your answer as:\\n1. [first way]\\n2. [second way]\\n3. [third way]\",\n",
    "            \"description\": \"Explicit list format task\",\n",
    "            \"max_tokens\": 80\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": \"In insurance, explain the difference between loss ratio and combined ratio. Loss ratio is... Combined ratio is...\",\n",
    "            \"description\": \"Structured comparison task\",\n",
    "            \"max_tokens\": 70\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for j, test in enumerate(improved_tests, 1):\n",
    "        print(f\"\\nüìã Improved Test {j}: {test['description']}\")\n",
    "        print(f\"Instruction: '{test['instruction']}'\")\n",
    "        \n",
    "        response = call_endpoint(endpoint_name, test['instruction'], max_new_tokens=test['max_tokens'])\n",
    "        \n",
    "        if response:\n",
    "            print(f\"Response: {response}\")\n",
    "            \n",
    "            # Evaluate improvement\n",
    "            if \"1.\" in response or \"2.\" in response or \"3.\" in response:\n",
    "                print(\"‚úÖ Excellent - followed numbered list format!\")\n",
    "            elif any(word in response.lower() for word in ['ratio', 'model', 'insurance', 'risk']):\n",
    "                print(\"‚úÖ Good - contains relevant insurance terms\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Basic response generated\")\n",
    "        else:\n",
    "            print(\"‚ùå No response generated\")\n",
    "        \n",
    "        print(\"-\" * 25)\n",
    "\n",
    "# Test with context-enhanced prompts\n",
    "def test_context_enhanced_prompts(endpoint_name):\n",
    "    \"\"\"Test using your RAG context for better responses\"\"\"\n",
    "    print(f\"\\nüéØ Context-Enhanced Instruction Tests\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Use your vector search to get context\n",
    "    context_query = \"What is loss ratio?\"\n",
    "    search_results = vector_search(context_query, \"loss-ratio\", top_k=2)\n",
    "    \n",
    "    if search_results:\n",
    "        context = search_results[0]['item']['text'][:200] + \"...\"\n",
    "        \n",
    "        enhanced_prompt = f\"\"\"Context: {context}\n",
    "\n",
    "Task: Based on the context above, list 3 key points about loss ratio in insurance.\n",
    "\n",
    "Format your answer as:\n",
    "1. [point one]\n",
    "2. [point two]  \n",
    "3. [point three]\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        print(\"üìù Testing context-enhanced prompt...\")\n",
    "        response = call_endpoint(endpoint_name, enhanced_prompt, max_new_tokens=90)\n",
    "        \n",
    "        if response:\n",
    "            print(f\"ü§ñ Context-Enhanced Response: {response}\")\n",
    "            if \"1.\" in response and \"2.\" in response:\n",
    "                print(\"‚úÖ Excellent - used context AND followed format!\")\n",
    "            else:\n",
    "                print(\"‚úÖ Good - generated relevant response\")\n",
    "        else:\n",
    "            print(\"‚ùå No response\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Could not retrieve context for enhanced test\")\n",
    "\n",
    "# Run the improved tests\n",
    "if 'DEPLOYED_ENDPOINT_NAME' in locals() and DEPLOYED_ENDPOINT_NAME:\n",
    "    test_improved_instructions(DEPLOYED_ENDPOINT_NAME)\n",
    "    test_context_enhanced_prompts(DEPLOYED_ENDPOINT_NAME)\n",
    "else:\n",
    "    print(\"No endpoint available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50955aff-b9ab-4ae6-9163-e1dd939c9dfc",
   "metadata": {},
   "source": [
    "# STEP 4: CRITICAL - Cleanup to avoid charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0918a90-e81f-45ed-b98b-9097f960d8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ STEP 4: Cleanup (CRITICAL for cost management)\n",
      "==================================================\n",
      "Current endpoint: tech-translator-gpt2-1748092833\n",
      "üìä Current status: InService\n",
      "‚úÖ Endpoint is running (and charging you money!)\n",
      "\n",
      "‚ö†Ô∏è  WARNING: You are about to delete endpoint: tech-translator-gpt2-1748092833\n",
      "This will stop all charges but you'll need to redeploy to continue testing.\n",
      "üóëÔ∏è  Proceeding with endpoint deletion...\n",
      "\n",
      "üßπ Cleaning up endpoint: tech-translator-gpt2-1748092833\n",
      "‚úÖ Endpoint deletion initiated!\n",
      "üí∞ This will stop incurring charges.\n",
      "‚úÖ Cleanup completed successfully!\n",
      "üí∞ Your endpoint is no longer incurring charges.\n",
      "üìù Endpoint variable cleared.\n",
      "\n",
      "üèÅ Session complete!\n",
      "Summary of what we accomplished:\n",
      "‚úÖ Deployed model to SageMaker\n",
      "‚úÖ Tested basic model functionality\n",
      "‚úÖ Integrated with RAG system\n",
      "‚úÖ Demonstrated prompt engineering\n",
      "‚úÖ Cleaned up resources to avoid charges\n",
      "\n",
      "Your TechTranslator system is working! üéâ\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: CRITICAL - Cleanup to avoid charges\n",
    "if 'DEPLOYED_ENDPOINT_NAME' in locals() and DEPLOYED_ENDPOINT_NAME:\n",
    "    print(\"üßπ STEP 4: Cleanup (CRITICAL for cost management)\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(f\"Current endpoint: {DEPLOYED_ENDPOINT_NAME}\")\n",
    "    \n",
    "    # Check endpoint status before cleanup\n",
    "    try:\n",
    "        sagemaker_client = boto3.client('sagemaker')\n",
    "        response = sagemaker_client.describe_endpoint(EndpointName=DEPLOYED_ENDPOINT_NAME)\n",
    "        status = response['EndpointStatus']\n",
    "        print(f\"üìä Current status: {status}\")\n",
    "        \n",
    "        if status == 'InService':\n",
    "            print(\"‚úÖ Endpoint is running (and charging you money!)\")\n",
    "        else:\n",
    "            print(f\"‚ÑπÔ∏è  Endpoint status: {status}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not check endpoint status: {str(e)}\")\n",
    "    \n",
    "    # Confirm cleanup\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: You are about to delete endpoint: {DEPLOYED_ENDPOINT_NAME}\")\n",
    "    print(\"This will stop all charges but you'll need to redeploy to continue testing.\")\n",
    "    \n",
    "    # For notebook use, we'll proceed with cleanup\n",
    "    # In production, you might want user confirmation\n",
    "    \n",
    "    print(\"üóëÔ∏è  Proceeding with endpoint deletion...\")\n",
    "    \n",
    "    if cleanup_endpoint(DEPLOYED_ENDPOINT_NAME):\n",
    "        print(\"‚úÖ Cleanup completed successfully!\")\n",
    "        print(\"üí∞ Your endpoint is no longer incurring charges.\")\n",
    "        \n",
    "        # Clear the variable\n",
    "        DEPLOYED_ENDPOINT_NAME = None\n",
    "        print(\"üìù Endpoint variable cleared.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Cleanup failed - you may need to delete manually from SageMaker console\")\n",
    "        print(f\"Endpoint name: {DEPLOYED_ENDPOINT_NAME}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No active endpoint found to cleanup.\")\n",
    "\n",
    "print(\"\\nüèÅ Session complete!\")\n",
    "print(\"Summary of what we accomplished:\")\n",
    "print(\"‚úÖ Deployed model to SageMaker\")\n",
    "print(\"‚úÖ Tested basic model functionality\") \n",
    "print(\"‚úÖ Integrated with RAG system\")\n",
    "print(\"‚úÖ Demonstrated prompt engineering\")\n",
    "print(\"‚úÖ Cleaned up resources to avoid charges\")\n",
    "print(\"\\nYour TechTranslator system is working! üéâ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
