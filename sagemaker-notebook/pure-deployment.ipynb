{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ab5a88-8504-4bea-98cc-7f3c28dd29e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.12/site-packages (1.37.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/lib/python3.12/site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.37.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.12/site-packages (from boto3) (0.11.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.1->boto3) (2.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.1.0)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.1->boto3) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy!=1.13.2,>=1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Installing collected packages: sentence-transformers\n",
      "Successfully installed sentence-transformers-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers boto3 numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2de4aeb-07ab-4228-859f-2ee87284ab06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-24 19:19:37.663015: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading dependencies and setting up clients...\n",
      "üìù Configuration:\n",
      "  S3 Bucket: tech-translator-s3-knowledge-base\n",
      "  DynamoDB Table: tech-translator-dynamodb-vector-storage\n",
      "ü§ñ Loading sentence transformer model...\n",
      "‚úÖ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Complete Testing Notebook - All Required Functions\n",
    "import boto3\n",
    "import json\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import logging\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"üì¶ Loading dependencies and setting up clients...\")\n",
    "\n",
    "# Initialize AWS clients\n",
    "s3 = boto3.client('s3')\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = \"arn:aws:iam::467383999568:role/LabRole\"  #get_execution_role()\n",
    "\n",
    "# Configuration - UPDATE THESE WITH YOUR VALUES\n",
    "BUCKET_NAME = \"tech-translator-s3-knowledge-base\"  # Your S3 bucket\n",
    "TABLE_NAME = \"tech-translator-dynamodb-vector-storage\"  # Your DynamoDB table\n",
    "\n",
    "print(f\"üìù Configuration:\")\n",
    "print(f\"  S3 Bucket: {BUCKET_NAME}\")\n",
    "print(f\"  DynamoDB Table: {TABLE_NAME}\")\n",
    "\n",
    "# Initialize sentence transformer model\n",
    "print(\"ü§ñ Loading sentence transformer model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# ===== DEPLOYMENT FUNCTIONS =====\n",
    "def deploy_model():\n",
    "    \"\"\"Deploy model to SageMaker endpoint \"\"\"\n",
    "    try:\n",
    "        print(\"üì¶ Creating HuggingFace model configuration...\")\n",
    "        \n",
    "        # Model configuration using env parameter\n",
    "        hub = {\n",
    "            'HF_MODEL_ID': 'google/flan-t5-large', #'google/flan-t5-base', #'google/flan-t5-small', \n",
    "            'HF_TASK': 'text2text-generation'\n",
    "        }\n",
    "        \n",
    "        # Create model with supported versions\n",
    "        huggingface_model = HuggingFaceModel(\n",
    "            transformers_version=\"4.37.0\",\n",
    "            pytorch_version=\"2.1.0\",\n",
    "            py_version=\"py310\",\n",
    "            env=hub,\n",
    "            role=role,\n",
    "        )\n",
    "        \n",
    "        print(\"üöÄ Deploying model...\")\n",
    "        print(\"‚è±Ô∏è  This may take 5-10 minutes...\")\n",
    "        \n",
    "        # List of instance types to try\n",
    "        instance_types = [\n",
    "            #\"ml.m5.large\",\n",
    "            #\"ml.c5.large\", \n",
    "            \"ml.m5.xlarge\",\n",
    "            \"ml.c5.xlarge\",\n",
    "        ]\n",
    "        \n",
    "        predictor = None\n",
    "        deployment_successful = False\n",
    "        \n",
    "        for instance_type in instance_types:\n",
    "            try:\n",
    "                print(f\"\\nüîÑ Trying deployment on {instance_type}...\")\n",
    "                \n",
    "                # FIXED: Proper endpoint name parameter\n",
    "                endpoint_name = f\"tech-translator-model-{int(time.time())}\"\n",
    "                \n",
    "                predictor = huggingface_model.deploy(\n",
    "                    initial_instance_count=1,\n",
    "                    instance_type=instance_type,\n",
    "                    endpoint_name=endpoint_name,  \n",
    "                    container_startup_health_check_timeout=600,\n",
    "                    model_data_download_timeout=600,\n",
    "                    wait=True\n",
    "                )\n",
    "                \n",
    "                print(f\"‚úÖ Successfully deployed on {instance_type}!\")\n",
    "                deployment_successful = True\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to deploy on {instance_type}: {str(e)}\")\n",
    "                if any(err in str(e) for err in [\"ResourceLimitExceeded\", \"InsufficientCapacity\", \"ValidationException\"]):\n",
    "                    print(\"   Trying next instance type...\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"   Unexpected error, continuing...\")\n",
    "                    continue\n",
    "        \n",
    "        if deployment_successful:\n",
    "            print(f\"‚úÖ Model deployed successfully!\")\n",
    "            print(f\"üìç Endpoint name: {endpoint_name}\")\n",
    "            return predictor, endpoint_name\n",
    "        else:\n",
    "            print(\"‚ùå All deployment attempts failed!\")\n",
    "            return None, None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model creation failed: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cleanup_endpoint(endpoint_name):\n",
    "    \"\"\"Delete the endpoint to avoid charges\"\"\"\n",
    "    print(f\"\\nüßπ Cleaning up endpoint: {endpoint_name}\")\n",
    "    \n",
    "    try:\n",
    "        sagemaker_client = boto3.client('sagemaker')\n",
    "        sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "        print(\"‚úÖ Endpoint deletion initiated!\")\n",
    "        print(\"üí∞ This will stop incurring charges.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error deleting endpoint: {str(e)}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec5a0794-a8a2-457b-937f-7141df8cf953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TESTING FUNCTIONS =====\n",
    "def call_endpoint(endpoint_name, prompt, max_new_tokens=100):\n",
    "    \"\"\"Helper function to call the deployed FLAN-T5 endpoint with proper format\"\"\"\n",
    "    \n",
    "    # FLAN-T5 is a text-to-text model, so we need to format the prompt properly\n",
    "    # It works best with instruction-style prompts\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": max_new_tokens,  # Use max_new_tokens instead of max_length\n",
    "            \"temperature\": 0.7,\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\": 0.9,\n",
    "            \"repetition_penalty\": 1.1,\n",
    "            # Remove return_full_text - not supported by FLAN-T5\n",
    "            # Remove pad_token_id - FLAN-T5 handles this automatically\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = sagemaker_runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response['Body'].read().decode())\n",
    "        \n",
    "        # Handle FLAN-T5 response format\n",
    "        if isinstance(result, list) and len(result) > 0:\n",
    "            if isinstance(result[0], dict):\n",
    "                generated_text = result[0].get('generated_text', '')\n",
    "            else:\n",
    "                generated_text = str(result[0])\n",
    "        elif isinstance(result, dict):\n",
    "            generated_text = result.get('generated_text', '')\n",
    "        else:\n",
    "            generated_text = str(result)\n",
    "        \n",
    "        # Clean up the response\n",
    "        generated_text = generated_text.strip()\n",
    "        \n",
    "        return generated_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Endpoint call failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "def quick_test(endpoint_name, test_query=\"What is R-squared?\"):\n",
    "    \"\"\"Quick test of the FLAN-T5 endpoint with proper instruction format\"\"\"\n",
    "    print(f\"‚ö° Quick Test: '{test_query}'\")\n",
    "    \n",
    "    # Format the query as an instruction for FLAN-T5\n",
    "    instruction_prompt = f\"Explain the following concept: {test_query}\"\n",
    "    \n",
    "    response = call_endpoint(endpoint_name, instruction_prompt, max_new_tokens=50)\n",
    "    \n",
    "    if response:\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"‚úÖ Endpoint is working!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå Test failed\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ce3370-9805-47b2-9db1-81490787714a",
   "metadata": {},
   "source": [
    "# STEP 1: Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e67e434-f9b0-43d7-99ae-e638a06ffc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STEP 1: Deploying model...\n",
      "==================================================\n",
      "üì¶ Creating HuggingFace model configuration...\n",
      "üöÄ Deploying model...\n",
      "‚è±Ô∏è  This may take 5-10 minutes...\n",
      "\n",
      "üîÑ Trying deployment on ml.m5.xlarge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-inference-2025-05-24-19-19-50-284\n",
      "INFO:sagemaker:Creating endpoint-config with name tech-translator-model-1748114390\n",
      "INFO:sagemaker:Creating endpoint with name tech-translator-model-1748114390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!‚úÖ Successfully deployed on ml.m5.xlarge!\n",
      "‚úÖ Model deployed successfully!\n",
      "üìç Endpoint name: tech-translator-model-1748114390\n",
      "\n",
      "‚úÖ Deployment successful!\n",
      "üìç Endpoint: tech-translator-model-1748114390\n",
      "üìù Stored in variable: DEPLOYED_ENDPOINT_NAME\n",
      "\n",
      "‚è≥ Waiting for endpoint to be fully ready...\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Deploy the model\n",
    "print(\"üöÄ STEP 1: Deploying model...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "predictor, endpoint_name = deploy_model()\n",
    "\n",
    "if predictor and endpoint_name:\n",
    "    print(f\"\\n‚úÖ Deployment successful!\")\n",
    "    print(f\"üìç Endpoint: {endpoint_name}\")\n",
    "    \n",
    "    # Store for later use\n",
    "    DEPLOYED_ENDPOINT_NAME = endpoint_name\n",
    "    print(f\"üìù Stored in variable: DEPLOYED_ENDPOINT_NAME\")\n",
    "    \n",
    "    # Wait for endpoint to be ready\n",
    "    print(\"\\n‚è≥ Waiting for endpoint to be fully ready...\")\n",
    "    time.sleep(60)  # Wait 1 minute for endpoint to stabilize\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Deployment failed!\")\n",
    "    print(\"Check error messages above and try troubleshooting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9268b7-68dc-462f-873f-ffe5cb43ac7d",
   "metadata": {},
   "source": [
    "# Step 2: Update Lambda with endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0deb36f2-df96-46a5-a68d-29b2e8178c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîÑ UPDATING LAMBDA WITH NEW ENDPOINT\n",
      "============================================================\n",
      "üîÑ Updating Lambda with endpoint: tech-translator-model-1748114390\n",
      "‚úÖ Found Lambda stack: tech-translator-lambda\n",
      "üîÑ Updating CloudFormation stack...\n",
      "‚úÖ Stack update initiated: arn:aws:cloudformation:us-east-1:467383999568:stack/tech-translator-lambda/15700260-35aa-11f0-b87c-0eb6554517e1\n",
      "‚è≥ Waiting for stack update to complete...\n",
      "‚úÖ Lambda updated successfully!\n",
      "üìç Lambda now uses endpoint: tech-translator-model-1748114390\n",
      "üîç Verifying Lambda environment variables...\n",
      "Current Lambda endpoint setting: tech-translator-model-1748114390\n",
      "‚úÖ Lambda environment variable updated correctly!\n",
      "\n",
      "üéâ Integration complete!\n",
      "‚úÖ SageMaker endpoint deployed\n",
      "‚úÖ Lambda function updated\n"
     ]
    }
   ],
   "source": [
    "def update_lambda_with_endpoint(endpoint_name, lambda_stack_name=\"tech-translator-lambda\", region=\"us-east-1\"):\n",
    "    \"\"\"\n",
    "    Update Lambda function with new SageMaker endpoint name\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Updating Lambda with endpoint: {endpoint_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize CloudFormation client\n",
    "        cf_client = boto3.client('cloudformation', region_name=region)\n",
    "        \n",
    "        # Check if stack exists\n",
    "        try:\n",
    "            cf_client.describe_stacks(StackName=lambda_stack_name)\n",
    "            print(f\"‚úÖ Found Lambda stack: {lambda_stack_name}\")\n",
    "        except cf_client.exceptions.ClientError:\n",
    "            print(f\"‚ùå Lambda stack '{lambda_stack_name}' not found!\")\n",
    "            print(\"Make sure you've deployed your Lambda functions first with ./deploy.sh\")\n",
    "            return False\n",
    "        \n",
    "        # Update the stack with new endpoint parameter\n",
    "        print(\"üîÑ Updating CloudFormation stack...\")\n",
    "        \n",
    "        response = cf_client.update_stack(\n",
    "            StackName=lambda_stack_name,\n",
    "            UsePreviousTemplate=True,  # Keep the same template\n",
    "            Parameters=[\n",
    "                {\n",
    "                    'ParameterKey': 'SageMakerEndpointName',\n",
    "                    'ParameterValue': endpoint_name\n",
    "                },\n",
    "                # Keep all other parameters the same\n",
    "                {\n",
    "                    'ParameterKey': 'ProjectName',\n",
    "                    'UsePreviousValue': True\n",
    "                },\n",
    "                {\n",
    "                    'ParameterKey': 'S3StackName', \n",
    "                    'UsePreviousValue': True\n",
    "                },\n",
    "                {\n",
    "                    'ParameterKey': 'DynamoDBStackName',\n",
    "                    'UsePreviousValue': True\n",
    "                },\n",
    "                {\n",
    "                    'ParameterKey': 'CognitoStackName',\n",
    "                    'UsePreviousValue': True\n",
    "                },\n",
    "                {\n",
    "                    'ParameterKey': 'LambdaCodeBucket',\n",
    "                    'UsePreviousValue': True\n",
    "                }\n",
    "            ],\n",
    "            Capabilities=['CAPABILITY_IAM', 'CAPABILITY_NAMED_IAM']\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Stack update initiated: {response['StackId']}\")\n",
    "        \n",
    "        # Wait for update to complete\n",
    "        print(\"‚è≥ Waiting for stack update to complete...\")\n",
    "        waiter = cf_client.get_waiter('stack_update_complete')\n",
    "        waiter.wait(StackName=lambda_stack_name)\n",
    "        \n",
    "        print(\"‚úÖ Lambda updated successfully!\")\n",
    "        print(f\"üìç Lambda now uses endpoint: {endpoint_name}\")\n",
    "        \n",
    "        # Verify the update\n",
    "        print(\"üîç Verifying Lambda environment variables...\")\n",
    "        verify_lambda_update(endpoint_name, lambda_stack_name, region)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Lambda update failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def verify_lambda_update(endpoint_name, lambda_stack_name, region=\"us-east-1\"):\n",
    "    \"\"\"\n",
    "    Verify that Lambda function has the correct endpoint name\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cf_client = boto3.client('cloudformation', region_name=region)\n",
    "        lambda_client = boto3.client('lambda', region_name=region)\n",
    "        \n",
    "        # Get Lambda function name from stack outputs\n",
    "        stack_response = cf_client.describe_stacks(StackName=lambda_stack_name)\n",
    "        outputs = stack_response['Stacks'][0].get('Outputs', [])\n",
    "        \n",
    "        main_function_name = None\n",
    "        for output in outputs:\n",
    "            if output['OutputKey'] == 'MainLambdaFunctionName':\n",
    "                main_function_name = output['OutputValue']\n",
    "                break\n",
    "        \n",
    "        if main_function_name:\n",
    "            # Get current environment variables\n",
    "            function_config = lambda_client.get_function_configuration(\n",
    "                FunctionName=main_function_name\n",
    "            )\n",
    "            \n",
    "            current_endpoint = function_config.get('Environment', {}).get('Variables', {}).get('SAGEMAKER_ENDPOINT')\n",
    "            \n",
    "            print(f\"Current Lambda endpoint setting: {current_endpoint}\")\n",
    "            \n",
    "            if current_endpoint == endpoint_name:\n",
    "                print(\"‚úÖ Lambda environment variable updated correctly!\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Environment variable may not be updated yet\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Could not find Lambda function name in stack outputs\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not verify update: {str(e)}\")\n",
    "\n",
    "# Usage after your model deployment\n",
    "if 'endpoint_name' in locals() and endpoint_name:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîÑ UPDATING LAMBDA WITH NEW ENDPOINT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    success = update_lambda_with_endpoint(endpoint_name)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nüéâ Integration complete!\")\n",
    "        print(\"‚úÖ SageMaker endpoint deployed\")\n",
    "        print(\"‚úÖ Lambda function updated\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Manual update may be needed\")\n",
    "        print(f\"Update CloudFormation parameter SageMakerEndpointName to: {endpoint_name}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No endpoint_name variable found!\")\n",
    "    print(\"Make sure you run this cell after successfully deploying your SageMaker endpoint\")\n",
    "    print(\"The endpoint_name should be available from your deployment cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75283c4-5f33-4822-af34-457f62432df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08393b-ba16-447c-9958-564ad21785fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b033c04a-8220-412c-a8f3-51540a6255b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
